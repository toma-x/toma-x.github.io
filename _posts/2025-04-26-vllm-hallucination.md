---
layout: post
title: Multimodal Hallucination Detector
---

Hey everyone,

So, after messing around with some LeetCode and trying to build that VQA agent, I got kinda fascinated (and sometimes frustrated) by how these big Vision Language Large Models (VLLMs) work. You see them generating captions for images, and sometimes they're spot on, but other times... they just make stuff up? Like, the caption mentions a dog playing fetch, but there's no dog in the picture. That phenomenon is apparently called "hallucination," and I thought it would be a cool project to try and build something that could automatically detect when a VLLM caption was hallucinating objects that weren't actually there.

My main idea was to dig into the Vision Transformer (ViT) part of these models. My hypothesis was pretty simple: if the model is generating a caption about, say, a "red car," then its attention mechanism *should* be focusing on the red car in the image when it's thinking about that part of the caption. If the attention maps show it looking somewhere completely different while talking about a car, maybe it's hallucinating? I decided to build this detector using **PyTorch** because I'm most familiar with it and it gives good access to model internals.

## The Plan: Using Attention Maps

So, the core idea was to use **attention map analysis**. ViTs process images by chopping them into patches and then using transformer layers (with self-attention) to figure out how different patches relate to each other. These attention mechanisms produce maps showing which parts of the image the model "focused" on.

My plan was:
1.  Take an image and a caption generated by some VLLM.
2.  Extract the attention maps from a pre-trained ViT when it processes that image.
3.  Identify the key objects mentioned in the caption (e.g., nouns like "cat," "tree," "bicycle").
4.  For each object noun, check if the ViT's attention maps strongly focus on the corresponding area in the image.
5.  If the caption mentions an object but the attention maps show the model wasn't really "looking" at it, flag it as a potential hallucination.

It sounded plausible enough, right?

## First Hurdle: Getting the Data

Okay, this was way harder than I thought. I needed image-caption pairs, but specifically, I needed pairs where I *knew* if the caption was factual or hallucinated. Standard datasets like COCO have images and (mostly) factual captions. Where do you get confirmed *hallucinated* captions?

I ended up doing a messy combination:
*   Used COCO captions as examples of (mostly) "non-hallucinated" captions.
*   Tried generating captions for COCO images using a separate, pre-trained VLLM captioning model I found online. I manually scrolled through *hundreds* of these generated captions, looking for obvious errors where it mentioned objects clearly not present. This was super tedious and subjective.
*   I only managed to gather a small set of maybe 50-100 examples that I felt confident were hallucinations. This tiny, manually curated dataset was definitely a weak point.

## Getting the Attention Maps with PyTorch

Next, I needed to actually extract the attention maps from a ViT. I used a standard pre-trained ViT model from Hugging Face's `transformers` library. Getting the attention weights usually involves using PyTorch hooks, which let you grab the outputs of specific layers during the forward pass.

Finding the *right* layer took some trial and error, printing out the model structure (`print(vit_model)`) and digging through nested modules. It often looks something like this, but the exact path (`encoder.layer[...].attention.attention`) changes depending on the specific ViT implementation:

```python
import torch
from transformers import ViTModel, ViTImageProcessor
from PIL import Image

# Load pre-trained ViT model and processor
model_name = "google/vit-base-patch16-224-in21k" # Using a base ViT for image processing
processor = ViTImageProcessor.from_pretrained(model_name)
model = ViTModel.from_pretrained(model_name, output_attentions=True) # Need to enable this!
model.eval()

# Store attention maps here
attention_maps_list = []

# --- Using the output_attentions=True flag is easier than hooks sometimes ---

def get_attention(image_path):
    """Processes an image and returns attention maps."""
    try:
        image = Image.open(image_path).convert("RGB")
        inputs = processor(images=image, return_tensors="pt")

        with torch.no_grad():
            outputs = model(**inputs)
        
        # attentions is a tuple of tensors, one for each layer
        # Each tensor shape: [batch_size, num_heads, seq_length, seq_length]
        # seq_length includes CLS token + patches
        attentions = outputs.attentions 
        return attentions # Return attentions from all layers
        
    except Exception as e:
        print(f"Error processing {image_path}: {e}")
        return None

# Example Usage:
img_path = 'path/to/your/image.jpg'
attentions = get_attention(img_path) 

# Now 'attentions' holds the attention maps from all layers
# We might average over heads or select specific layers/heads later
# e.g., attentions[-1] gives the maps from the last layer

```
*Self-correction: Initially, I thought about manual hooks, but realized many Hugging Face models have an `output_attentions=True` flag which is often easier! The shape `[batch_size, num_heads, seq_length, seq_length]` is key – it shows how much each token (patch) attends to every other token.*

## Linking Captions to Attention: The Real Challenge

This was where my simple idea started to fall apart. How do you connect a word like "dog" in the caption to specific attention patterns?

1.  **Extract Objects:** I used spaCy to pull out noun chunks from the caption. Simple enough. `("a red car", "the fluffy dog") -> ["red car", "fluffy dog"]`.
2.  **Analyze Attention:** The attention maps show patch-to-patch attention. Which patch(es) correspond to the "dog"? This is the core problem. ViT doesn't inherently know about "objects," just patches.
3.  **My (Flawed) Approach:** I tried a heuristic:
    *   Average the attention maps across all heads in the last layer to get a single map per image `[seq_length, seq_length]`.
    *   Look at the attention weights corresponding to the `[CLS]` token (often used for classification, maybe it summarizes image content?). See which patches the `[CLS]` token attends to the most. `att_map[0, 1:]` would give attention from CLS to patches.
    *   For each noun extracted ("dog"): Does the *word* "dog" have a high similarity (e.g., using CLIP text-image features) to the image patches that received the highest attention from the `[CLS]` token?
    *   If the noun has low similarity to the highly attended patches, maybe it's hallucinated?

Here's a conceptual snippet of that similarity check idea:

```python
# --- Conceptual Code ---
# Assume 'noun' is extracted, 'high_attention_patches' are feature vectors 
# of the most attended patches from ViT, and 'clip_model', 'clip_processor' are loaded.

def check_noun_patch_similarity(noun, high_attention_patches, clip_model, clip_processor):
    # Encode the noun text
    text_inputs = clip_processor(text=f"a photo of a {noun}", return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        text_features = clip_model.get_text_features(**text_inputs)
        text_features /= text_features.norm(dim=-1, keepdim=True) # Normalize

    # Assume high_attention_patches are already normalized image features [N_patches, feature_dim]
    # Calculate cosine similarity
    similarities = (text_features @ high_attention_patches.T) # Shape [1, N_patches]
    
    # Check if *any* highly attended patch strongly matches the noun
    max_similarity = similarities.max().item() 
    
    # Define a threshold (needs tuning!)
    threshold = 0.25 # Example threshold
    
    if max_similarity < threshold:
        print(f"Noun '{noun}' has low similarity ({max_similarity:.2f}) to highly attended patches. Potential hallucination?")
        return True # Potential hallucination
    else:
        print(f"Noun '{noun}' seems relevant to attended patches ({max_similarity:.2f}).")
        return False # Likely not hallucinated (based on this crude check)

# --- End Conceptual Code ---
```

## Faceplant Moments and Why This Was Hard

*   **Attention isn't Grounding:** ViT attention tells you which patches influence other patches' representations. It's not guaranteed to neatly highlight distinct "objects" in a way that directly maps to nouns, especially for abstract things ("sky," "background") or complex scenes. The `[CLS]` token attention was just one guess; maybe averaging all patch attentions would be better? I wasn't sure.
*   **CLIP Isn't Perfect:** Using CLIP similarity between the noun and attended patches was another layer of approximation. CLIP might find a patch vaguely similar even if the object isn't really there, or fail if the object looks unusual.
*   **Noisy Maps:** The raw attention maps were often messy. Averaging across heads helped, but sometimes the focus was still spread out or seemingly random.
*   **No Real Baseline:** Without a reliable dataset of hallucinations, how do you set the CLIP similarity `threshold`? I just picked values that seemed okay on my tiny manual set, which isn't rigorous at all.
*   **Complexity:** This heuristic felt very shaky. It ignores *where* in the caption the noun appears and how the VLLM might be using attention *during* generation (which I didn't even attempt to track).

## Results: Uh... It Kinda Worked? Sometimes?

Did it reliably detect hallucinations? Not really.
*   It *sometimes* caught very obvious errors, like if the caption said "There is an airplane" and the image was indoors. In these cases, the highly attended patches (maybe focused on furniture) would have very low CLIP similarity to "airplane."
*   But for more subtle things (e.g., wrong object color, mentioning a small background object that wasn't there), it often failed. The attention maps might still focus somewhere plausible, or the CLIP similarity check wasn't sensitive enough.
*   I tried calculating precision/recall on my tiny hand-labeled hallucination set (maybe 20-30 images I held out). The results were pretty poor – lots of false negatives (missing hallucinations) and some false positives (flagging correct captions).

## What I Learned (Mostly About What *Doesn't* Work Easily)

*   **Attention Maps are Complex:** They provide *some* insight but aren't a straightforward map of "what the model is looking at" in semantic terms. Interpreting them is hard.
*   **Multimodal Grounding is Hard:** Reliably connecting specific words in generated text back to specific regions or objects in an image is a major research challenge. My simple noun-patch similarity check was way too basic.
*   **Data is King (Again):** The lack of a good dataset for training or even just evaluating a hallucination detector was crippling. Without ground truth, it's just guesswork.
*   **PyTorch is Flexible:** Even though the results weren't great, PyTorch made it relatively easy to load models, extract internals like attention, and plug in other models like CLIP to experiment with these ideas.

## What I'd Do Differently Next Time

*   **Don't Rely Solely on Attention:** Explore other methods. Maybe look at the *consistency* between features extracted by ViT and features expected by the language model part when it generates certain words?
*   **Get Better Data:** Focus heavily on creating or finding a dataset with reliable hallucination labels, even if small and focused on specific *types* of hallucinations (e.g., only object presence/absence).
*   **Object Detection First?** Maybe run an object detector on the image first to get bounding boxes. Then, check if the nouns in the caption correspond to detected objects. If a captioned object isn't detected, *then* maybe look at attention or other features. Seems more robust than my fuzzy attention-patch approach.
*   **Simpler Task:** Start with a simpler problem, like detecting hallucinations in very short captions or only for specific object categories.

## Conclusion

So yeah, this project was a bit of a reality check. Trying to use ViT attention maps to detect VLLM caption hallucinations sounded cool, but turned out to be much harder than I expected. My heuristic approach was brittle and didn't work reliably. It highlighted how difficult it is to interpret these complex models and how crucial good data is. While I didn't build a great hallucination detector, I learned a lot about the guts of Vision Transformers, the challenges of multimodal grounding, and the limitations of simple interpretability methods. It was definitely frustrating at times, but a valuable exercise in seeing just how complex "understanding" is for these AI models.

Happy to hear if others have struggled with similar interpretability nightmares!