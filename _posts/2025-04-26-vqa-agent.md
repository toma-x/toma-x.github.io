---
layout: post
title: Building a Visual Question Answering Agent with ViT and Llama 2
---

Hey everyone,

After diving into those LeetCode problems, I switched gears quite a bit for my latest project. I wanted to try my hand at something in the multimodal AI space, specifically Visual Question Answering (VQA). The idea is simple to state: can you build an AI that answers questions about an image? But actually building it... well, that was a journey! I decided to combine a Vision Transformer (ViT) for image understanding and Llama 2 for language processing and reasoning.

## What's VQA Anyway?

Imagine showing your program a picture of a cat sitting on a mat and asking, "What color is the cat?" or "What is the cat sitting on?". The goal of VQA is to get the correct answer ("black", "mat"). It requires understanding the image content and relating it to the natural language question.

## Why ViT and Llama 2?

I chose ViT because it's become a really strong model for image classification and feature extraction, directly applying the transformer architecture (which revolutionized NLP) to vision. For the language part, Llama 2 is one of the powerful open-source large language models (LLMs) available, known for its reasoning and generation capabilities. My thinking was: use ViT to "see" the image and Llama 2 to "understand" the question and the visual context provided by ViT, then generate the answer.

## The High-Level Plan

Okay, so how do you actually connect an image model and a language model? This was the core challenge. My plan looked something like this:

1.  **Image Processing:** Feed the input image into a pre-trained ViT model. Instead of using the final classification output, I'd extract the image features from one of its intermediate or final layers – these features are like a dense numerical summary of the image content.
2.  **Text Processing:** Take the input question and tokenize it using the Llama 2 tokenizer.
3.  **Combining Modalities:** This is the tricky bit. I needed to make the image features understandable to Llama 2. The most common approach, and the one I tried, is to project the ViT image features into the same dimensional space as Llama 2's word embeddings. Think of it as translating the "image language" into "word language" that Llama 2 can process alongside the question text.
4.  **Input to Llama 2:** Create an input sequence for Llama 2 that includes both the projected image features and the tokenized question. For example, you might structure it like: `[Image Features] [Question Tokens]`.
5.  **Answer Generation:** Have Llama 2 process this combined input and generate the answer sequence, similar to how it would complete text.
6.  **Fine-tuning:** Since the pre-trained Llama 2 wasn't trained specifically for VQA or to understand these projected image features, the whole system (or at least parts of it) needs to be fine-tuned on a VQA dataset.

Here's a conceptual diagram:

```
[ Image ] --> [ Pre-trained ViT ] --> [ Image Features ] --+
                                                          |
                                                          V [ Projection Layer ] --> [ Features in LLM Space ] --+--> [ Llama 2 ] --> [ Answer ]
                                                                                                                 |
[ Question Text ] --> [ Llama 2 Tokenizer ] --> [ Question Tokens ] --------------------------------------------+
```

## The Dataset: VQAv2

For fine-tuning, I used the VQAv2 dataset. It's a standard benchmark for VQA, containing hundreds of thousands of images (from COCO), with multiple questions per image and multiple human-provided answers for each question (usually 10 answers). Having multiple answers is important because questions can be subjective or have several valid responses. The standard evaluation checks if the model's generated answer matches at least a certain number of the human answers.

Data preprocessing involved:
*   Images: Resizing and normalizing them according to ViT's requirements.
*   Text: Tokenizing questions and answers using the Llama 2 tokenizer. Preparing input sequences and target answer sequences for the fine-tuning process.

## Getting Hands-On with PyTorch

I used PyTorch and the Hugging Face `transformers` library, which makes loading pre-trained models like ViT and Llama 2 much easier.

**1. Loading Models (Conceptual):**

```python
import torch
from transformers import ViTModel, ViTImageProcessor, LlamaForCausalLM, LlamaTokenizer

# Load ViT model and processor
vit_model_name = "google/vit-base-patch16-224-in21k" # Example ViT
vit_processor = ViTImageProcessor.from_pretrained(vit_model_name)
vit_model = ViTModel.from_pretrained(vit_model_name)

# Load Llama model and tokenizer
llama_model_name = "meta-llama/Llama-2-7b-hf" # Example Llama 2
llama_tokenizer = LlamaTokenizer.from_pretrained(llama_model_name)
llama_model = LlamaForCausalLM.from_pretrained(llama_model_name)

# Add padding token if needed for Llama tokenizer
if llama_tokenizer.pad_token is None:
    llama_tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    llama_model.resize_token_embeddings(len(llama_tokenizer))

```

**2. Extracting Image Features:**

```python
def get_image_features(image, processor, model):
    # Preprocess image
    inputs = processor(images=image, return_tensors="pt")
    
    # Get features (e.g., last hidden state)
    with torch.no_grad(): # Don't need gradients here if ViT is frozen
        outputs = model(**inputs)
        # We might take the features of the [CLS] token or average pool
        features = outputs.last_hidden_state 
    return features
```

**3. The Projection Layer:**
I needed a layer to map ViT's output dimension to Llama 2's embedding dimension.

```python
vit_hidden_dim = vit_model.config.hidden_size
llama_hidden_dim = llama_model.config.hidden_size

# Simple linear layer
projection_layer = torch.nn.Linear(vit_hidden_dim, llama_hidden_dim)
```

**4. Combining Inputs for Llama 2 (Conceptual):**
This part required careful handling of embeddings and attention masks.

```python
def prepare_llama_input(image_features, question_text, tokenizer, projection):
    # Project image features
    projected_features = projection(image_features) # Shape: [batch_size, num_img_tokens, llama_dim]

    # Tokenize question
    question_tokens = tokenizer(question_text, return_tensors="pt", padding="longest", truncation=True)
    question_embeddings = llama_model.get_input_embeddings()(question_tokens.input_ids) # Shape: [batch_size, num_q_tokens, llama_dim]

    # Combine embeddings (e.g., concatenate)
    combined_embeddings = torch.cat([projected_features, question_embeddings], dim=1)
    
    # Create combined attention mask
    # (Needs careful construction: image tokens attend to each other and question tokens,
    # question tokens attend to image tokens and each other)
    # ... complex attention mask creation ...
    
    # Placeholder for attention mask logic
    combined_attention_mask = torch.ones(combined_embeddings.shape[:2], dtype=torch.long) 

    return combined_embeddings, combined_attention_mask
```
*(Note: Creating the correct attention mask for combined multimodal inputs is non-trivial!)*

**5. Fine-Tuning Loop (Simplified):**

```python
# Freeze most parameters
for param in vit_model.parameters():
    param.requires_grad = False
for param in llama_model.parameters(): # Freeze most of Llama too
    param.requires_grad = False 
# Unfreeze specific Llama layers if desired (e.g., adapters, last layers)
# projection_layer parameters are trainable by default

# Setup optimizer (only for trainable parameters)
trainable_params = list(projection_layer.parameters()) # + any unfrozen Llama params
optimizer = torch.optim.AdamW(trainable_params, lr=1e-4) 

# Example training step
def training_step(batch, model, tokenizer, projection, optimizer):
    images = batch['image'] # Assume batch has pre-loaded images
    questions = batch['question']
    answers = batch['answer'] # Target answers

    image_features = get_image_features(images, vit_processor, vit_model)
    
    # Prepare input embeddings and mask
    input_embeddings, attention_mask = prepare_llama_input(image_features, questions, tokenizer, projection)
    
    # Prepare target labels (shift for causal LM)
    answer_tokens = tokenizer(answers, return_tensors="pt", padding="longest", truncation=True)
    labels = answer_tokens.input_ids
    # Shift labels: prediction for token i uses inputs up to i
    labels[labels == tokenizer.pad_token_id] = -100 # Ignore padding in loss

    optimizer.zero_grad()
    
    # Forward pass using embeddings
    outputs = model(inputs_embeds=input_embeddings, 
                    attention_mask=attention_mask, 
                    labels=labels)
    
    loss = outputs.loss
    loss.backward()
    optimizer.step()
    
    return loss.item()

# --- Run this loop over the VQAv2 training set ---
```

## Challenges Encountered

*   **Connecting ViT & Llama 2:** Getting the projection right and ensuring Llama 2 actually paid attention to the image features was tough. Early attempts sometimes resulted in the model ignoring the image and just answering based on the question's general knowledge. I experimented with different ways to inject the visual features and tweaked the projection layer. Freezing most of the LLM and only training the projection layer (and maybe a few LLM layers) seemed crucial to avoid catastrophic forgetting and keep training manageable.
*   **GPU Memory:** Oh boy. Llama 2 (even the 7B parameter version) is *huge*. ViT isn't small either. Fitting the models, activations, and gradients into GPU memory was a constant battle. I relied heavily on Google Colab Pro GPUs, used gradient accumulation (processing smaller batches and accumulating gradients before updating weights), and mixed-precision training (`torch.cuda.amp`) to make it feasible. Even then, training was slow.
*   **Hyperparameter Tuning:** Finding the right learning rate was critical. Too high, and the training diverged; too low, and it took forever to learn. I mostly stuck to small learning rates (like 1e-4 or 5e-5) for the projection layer and any unfrozen LLM parts.
*   **VQAv2 Evaluation:** The official VQAv2 evaluation metric involves checking agreement with multiple human answers. Implementing this correctly requires careful handling of the answer list for each question. For quick checks during training, I often used simple exact match accuracy on the most common answer, but for final results, the official metric is needed.

## Results and Examples

After a lot of fine-tuning and debugging, I started getting decent results! On the VQAv2 validation set, my agent achieved accuracy scores that were "competitive" – maybe not state-of-the-art compared to massive models trained by big research labs, but definitely showing it learned the task. It was correctly answering questions that required integrating visual information and language understanding.

**Example Success:**
*Image:* A photo of two zebras standing in a grassy field.
*Question:* "How many zebras are there?"
*Generated Answer:* "2"

**Example Imperfection:**
*Image:* A picture of a complex street scene with cars, pedestrians, and traffic lights.
*Question:* "What color is the traffic light?" (Maybe it's small or ambiguous in the image).
*Generated Answer:* "green" (Even if it was red, perhaps defaulting to a common state).

Failures often happened with very detailed spatial reasoning questions, counting many small objects, or reading text within the image (which these models aren't typically great at unless specifically trained for OCR).

## What I Learned

This project taught me a ton:
*   **Multimodal is Fascinating (and Hard):** Combining vision and language models opens up cool possibilities but presents unique challenges in fusing information effectively.
*   **Leveraging Pre-trained Models:** Using ViT and Llama 2 as backbones saved immense effort. Fine-tuning is powerful but requires careful setup.
*   **Resource Management:** Dealing with large models hammered home the importance of efficient coding, gradient accumulation, and mixed precision.
*   **PyTorch in Practice:** I got much more familiar with debugging PyTorch training loops, handling different data types, and managing device memory (CPU/GPU).
*   **Attention is (Still) All You Need (Almost):** Understanding how attention mechanisms work within transformers, and how to potentially adapt them for multimodal inputs, was key.

## Final Thoughts

Building this VQA agent was one of the most challenging but rewarding projects I've worked on. It involved pulling together concepts from computer vision and NLP, wrestling with big models and limited resources, and diving deep into the practicalities of fine-tuning. While there's always room for improvement (trying different fusion techniques, larger models, maybe incorporating object detection explicitly), getting a ViT+Llama 2 combo to answer questions about images felt like a significant step. It definitely sparked my interest in pursuing multimodal AI further!

Happy to discuss if you have questions!
